digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140373010272640 [label="
 (100, 64, 21, 21)" fillcolor=darkolivegreen1]
	140373010192416 [label=LeakyReluBackward1]
	140373010194096 -> 140373010192416
	140373010194096 [label=CudnnBatchNormBackward0]
	140373010191696 -> 140373010194096
	140373010191696 [label=ConvolutionBackward0]
	140373010191600 -> 140373010191696
	140373010191600 [label=LeakyReluBackward1]
	140373010207792 -> 140373010191600
	140373010207792 [label=CudnnBatchNormBackward0]
	140373010209040 -> 140373010207792
	140373010209040 [label=ConvolutionBackward0]
	140373010207408 -> 140373010209040
	140373010207408 [label=MaxPool2DWithIndicesBackward0]
	140373010207504 -> 140373010207408
	140373010207504 [label=LeakyReluBackward1]
	140373010208992 -> 140373010207504
	140373010208992 [label=CudnnBatchNormBackward0]
	140373010209136 -> 140373010208992
	140373010209136 [label=ConvolutionBackward0]
	140373010207744 -> 140373010209136
	140373010207744 [label=MaxPool2DWithIndicesBackward0]
	140373010207696 -> 140373010207744
	140373010207696 [label=LeakyReluBackward1]
	140373010209424 -> 140373010207696
	140373010209424 [label=CudnnBatchNormBackward0]
	140373010232368 -> 140373010209424
	140373010232368 [label=ConvolutionBackward0]
	140373010234576 -> 140373010232368
	140371330363456 [label="0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	140371330363456 -> 140373010234576
	140373010234576 [label=AccumulateGrad]
	140373010234288 -> 140373010209424
	140371330363280 [label="1.weight
 (64)" fillcolor=lightblue]
	140371330363280 -> 140373010234288
	140373010234288 [label=AccumulateGrad]
	140373010231744 -> 140373010209424
	140371330363536 [label="1.bias
 (64)" fillcolor=lightblue]
	140371330363536 -> 140373010231744
	140373010231744 [label=AccumulateGrad]
	140373010208896 -> 140373010209136
	140371330364016 [label="4.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140371330364016 -> 140373010208896
	140373010208896 [label=AccumulateGrad]
	140373010209568 -> 140373010208992
	140371330364096 [label="5.weight
 (64)" fillcolor=lightblue]
	140371330364096 -> 140373010209568
	140373010209568 [label=AccumulateGrad]
	140373010210720 -> 140373010208992
	140371330364176 [label="5.bias
 (64)" fillcolor=lightblue]
	140371330364176 -> 140373010210720
	140373010210720 [label=AccumulateGrad]
	140373010210048 -> 140373010209040
	140371330364576 [label="8.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140371330364576 -> 140373010210048
	140373010210048 [label=AccumulateGrad]
	140373010208368 -> 140373010207792
	140371330364656 [label="9.weight
 (64)" fillcolor=lightblue]
	140371330364656 -> 140373010208368
	140373010208368 [label=AccumulateGrad]
	140373010207168 -> 140373010207792
	140371330364736 [label="9.bias
 (64)" fillcolor=lightblue]
	140371330364736 -> 140373010207168
	140373010207168 [label=AccumulateGrad]
	140373010191312 -> 140373010191696
	140371330365136 [label="11.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140371330365136 -> 140373010191312
	140373010191312 [label=AccumulateGrad]
	140373010190640 -> 140373010194096
	140371330365216 [label="12.weight
 (64)" fillcolor=lightblue]
	140371330365216 -> 140373010190640
	140373010190640 [label=AccumulateGrad]
	140373010192944 -> 140373010194096
	140371330365296 [label="12.bias
 (64)" fillcolor=lightblue]
	140371330365296 -> 140373010192944
	140373010192944 [label=AccumulateGrad]
	140373010192416 -> 140373010272640
}
